from __future__ import annotations
import argparse
import json
import urllib.request
from pathlib import Path
from typing import Iterable, Iterator, Any, Dict
from datetime import datetime

from app.db.init_db import init_db
from app.db.base import SessionLocal
from app.db.models import MetricPoint
# å¯¼å…¥ registry é‡Œçš„é…ç½®
from app.registry import METRIC_FILES, ensure_supported

# æ¨¡æ‹Ÿæµè§ˆå™¨ UA
HEADERS = {'User-Agent': 'Mozilla/5.0'}

def _parse_metrics(value: str) -> list[str]:
    # å‡çº§ç‚¹ 1: æ”¯æŒ 'all' å…³é”®å­—
    if value.lower() == "all":
        return list(METRIC_FILES.keys())
    return [item.strip() for item in value.split(",") if item.strip()]

def fetch_raw_json(owner: str, repo: str, filename: str) -> Dict | None:
    """
    å‡çº§ç‚¹ 2: å¼ºå£®çš„ä¸‹è½½å™¨
    ä½¿ç”¨ urllib ç›´æŽ¥ä¸‹è½½ï¼Œé‡åˆ° 404 è‡ªåŠ¨æ•èŽ·å¼‚å¸¸ï¼Œä¸ä¼šè®©ç¨‹åºå´©æºƒã€‚
    """
    url = f"https://oss.x-lab.info/open_digger/github/{owner}/{repo}/{filename}"
    try:
        req = urllib.request.Request(url, headers=HEADERS)
        with urllib.request.urlopen(req, timeout=15) as resp:
            return json.loads(resp.read().decode())
    except urllib.error.HTTPError as e:
        if e.code == 404:
            print(f"   âš ï¸  [404] è¯¥ä»“åº“æ²¡æœ‰æ­¤æŒ‡æ ‡: {filename} (å·²è·³è¿‡)")
        else:
            print(f"   âŒ [HTTP Error] ä¸‹è½½å¤±è´¥ {filename}: {e.code}")
        return None
    except Exception as e:
        print(f"   âŒ [Error] ç½‘ç»œæˆ–å…¶ä»–é”™è¯¯ {filename}: {e}")
        return None

def parse_opendigger_data(raw_data: Dict) -> Dict[str, float]:
    """
    å‡çº§ç‚¹ 3: æ™ºèƒ½è§£æžå™¨
    å¤„ç† OpenDigger å„ç§å¥‡è‘©çš„è¿”å›žæ ¼å¼ (åˆ—è¡¨ã€å­—å…¸ã€åµŒå¥—avg)
    """
    result = {}
    
    # è‡ªåŠ¨è¯†åˆ«æ•°æ®æ˜¯åœ¨æ ¹ç›®å½•ï¼Œè¿˜æ˜¯åœ¨ 'avg'/'sum' é‡Œé¢
    target_dict = raw_data
    if "avg" in raw_data and isinstance(raw_data["avg"], dict):
        target_dict = raw_data["avg"]
    elif "sum" in raw_data and isinstance(raw_data["sum"], dict):
        target_dict = raw_data["sum"]
        
    for key, val in target_dict.items():
        # è¿‡æ»¤æŽ‰éžæ—¥æœŸ key (æ¯”å¦‚ "2023", "meta" ç­‰)
        # æœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ "YYYY-MM" (é•¿åº¦7, ä¸­é—´æ˜¯æ¨ªæ )
        if len(key) != 7 or key[4] != '-': 
            continue
            
        numeric_val = 0.0
        # æ•°æ®æ¸…æ´—ï¼šè½¬æˆ float
        if isinstance(val, (int, float)):
            numeric_val = float(val)
        elif isinstance(val, list):
            numeric_val = float(len(val)) # åˆ—è¡¨è½¬é•¿åº¦
            
        # è¡¥å…¨æ—¥æœŸä¸º YYYY-MM-01
        full_date = f"{key}-01"
        result[full_date] = numeric_val
        
    return result

def fetch_metrics(repo: str, metrics: Iterable[str]) -> dict[str, int]:
    owner, name = repo.split("/", 1)
    counts: dict[str, int] = {}
    
    with SessionLocal() as db:
        for metric in metrics:
            metric_file = METRIC_FILES.get(metric)
            if not metric_file: continue

            # 1. å®‰å…¨ä¸‹è½½ (é‡åˆ° 404 ä¼šè¿”å›ž Noneï¼Œä¸ä¼šå´©)
            raw_data = fetch_raw_json(owner, name, metric_file)
            if not raw_data: 
                continue

            # 2. æ™ºèƒ½è§£æž
            parsed_data = parse_opendigger_data(raw_data)
            if not parsed_data: 
                continue

            counts[metric] = 0
            
            # 3. å…¥åº“
            for date_str, value in parsed_data.items():
                dt_obj = datetime.strptime(date_str, "%Y-%m-%d").date()
                
                row = (
                    db.query(MetricPoint)
                    .filter(
                        MetricPoint.repo == repo,
                        MetricPoint.metric == metric,
                        MetricPoint.dt == dt_obj,
                    )
                    .first()
                )
                if row:
                    row.value = value
                else:
                    db.add(
                        MetricPoint(
                            repo=repo,
                            metric=metric,
                            dt=dt_obj,
                            value=value,
                        )
                    )
                counts[metric] += 1
        db.commit()
    return counts

def _iter_repos(repos_file: Path) -> Iterator[str]:
    seen: set[str] = set()
    if not repos_file.exists(): return
    with repos_file.open("r", encoding="utf-8") as handle:
        for line in handle:
            repo = line.strip()
            if not repo or repo.startswith("#") or repo in seen: continue
            seen.add(repo)
            yield repo

def _load_resume_marker(state_file: Path | None, resume: bool) -> str | None:
    if not resume or state_file is None or not state_file.exists(): return None
    return state_file.read_text(encoding="utf-8").strip() or None

def _store_resume_marker(state_file: Path | None, repo: str) -> None:
    if state_file is None: return
    state_file.write_text(repo, encoding="utf-8")

def main() -> None:
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--repo", help="owner/repo")
    group.add_argument("--repos-file", type=Path)
    parser.add_argument("--metrics", default="openrank,activity,attention", help="comma-separated metrics or 'all'")
    parser.add_argument("--state-file", type=Path)
    parser.add_argument("--resume", action="store_true")
    args = parser.parse_args()

    init_db()

    # è§£æž metrics (å¤„ç† 'all')
    metrics = _parse_metrics(args.metrics)
    
    # æ­¤æ—¶ metrics å·²ç»æ˜¯å®Œæ•´çš„åˆ—è¡¨äº†ï¼Œå¯ä»¥ç›´æŽ¥ç¡®ä¿æ”¯æŒ
    ensure_supported(metrics)

    if args.repo:
        print(f"ðŸš€ æ­£åœ¨å¤„ç† {args.repo} (å…± {len(metrics)} ä¸ªæŒ‡æ ‡)...")
        counts = fetch_metrics(args.repo, metrics)
        print(f"âœ… å®Œæˆ: {counts}")
        return

    repos_file: Path = args.repos_file
    resume_marker = _load_resume_marker(args.state_file, args.resume)
    skipping = resume_marker is not None
    
    for repo in _iter_repos(repos_file):
        if skipping:
            if repo == resume_marker: skipping = False
            continue
        print(f"ðŸš€ æ­£åœ¨å¤„ç† {repo}...")
        counts = fetch_metrics(repo, metrics)
        _store_resume_marker(args.state_file, repo)
        print(f"   -> {counts}")

if __name__ == "__main__":
    main()