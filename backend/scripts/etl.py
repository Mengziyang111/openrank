from __future__ import annotations
import argparse
import json
import urllib.request
from pathlib import Path
from typing import Iterable, Iterator, Any, Dict
from datetime import datetime

from app.db.init_db import init_db
from app.db.base import SessionLocal
from app.db.models import MetricPoint
import re
from sqlalchemy import text
# å¯¼å…¥ registry é‡Œçš„é…ç½®
from app.registry import METRIC_FILES, ensure_supported

# æ¨¡æ‹Ÿæµè§ˆå™¨ UA
HEADERS = {'User-Agent': 'Mozilla/5.0'}

def _parse_metrics(value: str) -> list[str]:
    # å‡çº§ç‚¹ 1: æ”¯æŒ 'all' å…³é”®å­—
    if value.lower() == "all":
        return list(METRIC_FILES.keys())
    return [item.strip() for item in value.split(",") if item.strip()]

def fetch_raw_json(owner: str, repo: str, filename: str) -> Dict | None:
    """
    å‡çº§ç‚¹ 2: å¼ºå£®çš„ä¸‹è½½å™¨
    ä½¿ç”¨ urllib ç›´æ¥ä¸‹è½½ï¼Œé‡åˆ° 404 è‡ªåŠ¨æ•è·å¼‚å¸¸ï¼Œä¸ä¼šè®©ç¨‹åºå´©æºƒã€‚
    """
    url = f"https://oss.open-digger.cn/github/{owner}/{repo}/{filename}"
    try:
        req = urllib.request.Request(url, headers=HEADERS)
        with urllib.request.urlopen(req, timeout=15) as resp:
            return json.loads(resp.read().decode())
    except urllib.error.HTTPError as e:
        if e.code == 404:
            print(f"   âš ï¸  [404] è¯¥ä»“åº“æ²¡æœ‰æ­¤æŒ‡æ ‡: {filename} (å·²è·³è¿‡)")
        else:
            print(f"   âŒ [HTTP Error] ä¸‹è½½å¤±è´¥ {filename}: {e.code}")
        return None
    except Exception as e:
        print(f"   âŒ [Error] ç½‘ç»œæˆ–å…¶ä»–é”™è¯¯ {filename}: {e}")
        return None

def parse_opendigger_data(raw_data: Dict) -> Dict[str, float]:
    """
    å‡çº§ç‚¹ 3: æ™ºèƒ½è§£æå™¨
    å¤„ç† OpenDigger å„ç§å¥‡è‘©çš„è¿”å›æ ¼å¼ (åˆ—è¡¨ã€å­—å…¸ã€åµŒå¥—avg)
    """
    result = {}
    
    # è‡ªåŠ¨è¯†åˆ«æ•°æ®æ˜¯åœ¨æ ¹ç›®å½•ï¼Œè¿˜æ˜¯åœ¨ 'avg'/'sum' é‡Œé¢
    target_dict = raw_data
    if "avg" in raw_data and isinstance(raw_data["avg"], dict):
        target_dict = raw_data["avg"]
    elif "sum" in raw_data and isinstance(raw_data["sum"], dict):
        target_dict = raw_data["sum"]
        
    for key, val in target_dict.items():
        # è¿‡æ»¤æ‰éæ—¥æœŸ key (æ¯”å¦‚ "2023", "meta" ç­‰)
        # æœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ "YYYY-MM" (é•¿åº¦7, ä¸­é—´æ˜¯æ¨ªæ )
        if len(key) != 7 or key[4] != '-': 
            continue
            
        numeric_val = 0.0
        # æ•°æ®æ¸…æ´—ï¼šè½¬æˆ float
        if isinstance(val, (int, float)):
            numeric_val = float(val)
        elif isinstance(val, list):
            numeric_val = float(len(val)) # åˆ—è¡¨è½¬é•¿åº¦
            
        # è¡¥å…¨æ—¥æœŸä¸º YYYY-MM-01
        full_date = f"{key}-01"
        result[full_date] = numeric_val
        
    return result

def fetch_metrics(repo: str, metrics: Iterable[str]) -> dict[str, int]:
    owner, name = repo.split("/", 1)
    counts: dict[str, int] = {}
    
    with SessionLocal() as db:
        # detect if legacy (metric,value) columns exist
        col_check = db.execute(
            text("SELECT column_name FROM information_schema.columns WHERE table_name='metric_points' AND column_name IN ('metric','value');")
        ).fetchall()
        has_metric_value = len(col_check) > 0

        for metric in metrics:
            metric_file = METRIC_FILES.get(metric)
            if not metric_file: continue

            # 1. å®‰å…¨ä¸‹è½½ (é‡åˆ° 404 ä¼šè¿”å› Noneï¼Œä¸ä¼šå´©)
            raw_data = fetch_raw_json(owner, name, metric_file)
            if not raw_data: 
                continue

            # 2. æ™ºèƒ½è§£æ
            parsed_data = parse_opendigger_data(raw_data)
            if not parsed_data: 
                continue

            counts[metric] = 0

            # 3. å…¥åº“ï¼šå…¼å®¹ä¸¤ç§ schema
            for date_str, value in parsed_data.items():
                dt_obj = datetime.strptime(date_str, "%Y-%m-%d").date()

                if has_metric_value:
                    row = (
                        db.query(MetricPoint)
                        .filter(
                            MetricPoint.repo == repo,
                            MetricPoint.metric == metric,
                            MetricPoint.dt == dt_obj,
                        )
                        .first()
                    )
                    if row:
                        row.value = value
                    else:
                        db.add(
                            MetricPoint(
                                repo=repo,
                                metric=metric,
                                dt=dt_obj,
                                value=value,
                            )
                        )
                else:
                    # write into metric_<safe> column; create column if necessary
                    safe = _sanitize_identifier(metric)
                    col = f"metric_{safe}"
                    db.execute(text(f"ALTER TABLE metric_points ADD COLUMN IF NOT EXISTS {col} double precision;"))

                    # try update
                    upd = db.execute(
                        text(f"UPDATE metric_points SET {col} = :value WHERE repo = :repo AND dt = :dt"),
                        {"value": value, "repo": repo, "dt": dt_obj},
                    )
                    if upd.rowcount == 0:
                        # insert a minimal row
                        ins_cols = "repo, dt, " + col
                        ins_sql = text(f"INSERT INTO metric_points ({ins_cols}) VALUES (:repo, :dt, :value)")
                        db.execute(ins_sql, {"repo": repo, "dt": dt_obj, "value": value})

                counts[metric] += 1
        db.commit()
    return counts


def _sanitize_identifier(name: str) -> str:
    # keep letters, numbers and underscore
    return re.sub(r"[^0-9a-zA-Z_]", "_", name)


# ä¿®æ”¹ backend/scripts/etl.py ä¸­çš„ sync_repo_table å‡½æ•°

def sync_repo_table(repo: str, metrics: Iterable[str]) -> None:
    sanitized = _sanitize_identifier(repo.replace('/', '_'))
    table_name = f"repo_{sanitized}"

    # 1) å®šä¹‰æˆ‘ä»¬è¦åŒæ­¥çš„æ‰€æœ‰åˆ—ï¼šåŸå§‹æŒ‡æ ‡ + æ ¸å¿ƒç»´åº¦å¾—åˆ†
    # åŸå§‹æŒ‡æ ‡åˆ—å
    metric_cols = [f"metric_{_sanitize_identifier(m)}" for m in metrics]
    
    # æ ¸å¿ƒå¾—åˆ†åˆ—å (å¯¹åº” health_overview_daily ä¸­çš„å­—æ®µ)
    score_cols = [
        "score_health", "score_vitality", "score_responsiveness", 
        "score_resilience", "score_governance", "score_security"
    ]
    
    # åˆå¹¶æ‰€æœ‰ç›®æ ‡åˆ—
    all_target_cols = metric_cols + score_cols

    with SessionLocal() as db:
        # 2) ç¡®ä¿è¡¨å­˜åœ¨
        db.execute(text(f"CREATE TABLE IF NOT EXISTS public.{table_name} (dt date PRIMARY KEY, repo_full_name text);"))

        # 3) ç¡®ä¿æ‰€æœ‰åˆ—ï¼ˆæŒ‡æ ‡åˆ— + å¾—åˆ†åˆ—ï¼‰éƒ½åœ¨è¡¨ä¸­å­˜åœ¨
        for col in all_target_cols:
            db.execute(text(f"ALTER TABLE public.{table_name} ADD COLUMN IF NOT EXISTS {col} double precision;"))

        # 4) æ„å»ºå¤æ‚çš„åŒæ­¥ SQL
        # æˆ‘ä»¬é€šè¿‡ LEFT JOIN æŠŠ metric_points çš„èšåˆæ•°æ®å’Œ health_overview_daily çš„å¾—åˆ†æ•°æ®åˆå¹¶
        cols_csv = ", ".join(all_target_cols)
        
        # æŒ‡æ ‡éƒ¨åˆ†ä» metric_points èšåˆ (MAX)ï¼Œå¾—åˆ†éƒ¨åˆ†ä» health_overview_daily ç›´æ¥å–
        select_metrics = ", ".join([f"max(mp.metric_{_sanitize_identifier(m)})" for m in metrics])
        select_scores = ", ".join([f"max(ho.{s})" for s in score_cols])
        
        update_csv = ", ".join([f"{col} = EXCLUDED.{col}" for col in all_target_cols])

        insert_sql = f"""
        INSERT INTO public.{table_name} (dt, repo_full_name, {cols_csv})
        SELECT 
            mp.dt, 
            :repo_full_name,
            {select_metrics}{', ' if select_metrics and select_scores else ''}{select_scores}
        FROM metric_points mp
        LEFT JOIN health_overview_daily ho 
            ON mp.repo = ho.repo_full_name AND mp.dt = ho.dt
        WHERE mp.repo = :repo
        GROUP BY mp.dt, ho.repo_full_name
        ON CONFLICT (dt) DO UPDATE SET {update_csv}, repo_full_name = EXCLUDED.repo_full_name;
        """

        db.execute(text(insert_sql), {"repo": repo, "repo_full_name": repo})
        db.commit()
        print(f"   âœ… synced per-repo table public.{table_name} (including health scores)")


def backfill_health_overview(repo: str, metrics: Iterable[str], limit_months: int | None = None) -> int:
    """Upsert health_overview_daily for given repo using data in metric_points.

    Supports both schemas of metric_points:
    - legacy: rows with columns (repo, metric, dt, value)
    - wide:   rows with columns repo, dt, metric_<name>
    """
    with SessionLocal() as db:
        # detect schema
        col_check = db.execute(
            text("SELECT column_name FROM information_schema.columns WHERE table_name='metric_points' AND column_name IN ('metric','value');")
        ).fetchall()
        has_metric_value = len(col_check) > 0

        # get distinct dates for repo
        dates = db.execute(
            text("SELECT DISTINCT dt FROM metric_points WHERE repo = :repo ORDER BY dt"), {"repo": repo}
        ).fetchall()
        dts = [row[0] for row in dates]
        if limit_months is not None:
            dts = dts[-int(limit_months):]

        from app.services.metric_engine import MetricEngine
        engine = MetricEngine()
        upserts = 0

        for dt_value in dts:
            metrics_dict: Dict[str, Any] = {}
            if has_metric_value:
                # collect metric/value rows
                rows = db.query(MetricPoint).filter(
                    MetricPoint.repo == repo, MetricPoint.dt == dt_value
                ).all()
                metrics_dict = {r.metric: r.value for r in rows}
            else:
                # read a single wide row and map columns back to metric keys
                row = db.execute(
                    text("SELECT * FROM metric_points WHERE repo = :repo AND dt = :dt LIMIT 1"),
                    {"repo": repo, "dt": dt_value},
                ).mappings().first()
                if row:
                    for m in metrics:
                        safe = _sanitize_identifier(m)
                        col = f"metric_{safe}"
                        if col in row and row[col] is not None:
                            metrics_dict[m] = float(row[col])

            if not metrics_dict:
                continue

            record = engine.compute(
                repo_full_name=repo,
                dt_value=dt_value,
                metrics=metrics_dict,
                governance_files={},
                scorecard_checks={},
            )
            engine.upsert(db, record)
            upserts += 1

        print(f"   âœ… backfilled health_overview_daily for {repo} ({upserts} snapshots)")
        return upserts
        
def _iter_repos(repos_file: Path) -> Iterator[str]:
    seen: set[str] = set()
    if not repos_file.exists(): return
    with repos_file.open("r", encoding="utf-8") as handle:
        for line in handle:
            repo = line.strip()
            if not repo or repo.startswith("#") or repo in seen: continue
            seen.add(repo)
            yield repo

def _load_resume_marker(state_file: Path | None, resume: bool) -> str | None:
    if not resume or state_file is None or not state_file.exists(): return None
    return state_file.read_text(encoding="utf-8").strip() or None

def _store_resume_marker(state_file: Path | None, repo: str) -> None:
    if state_file is None: return
    state_file.write_text(repo, encoding="utf-8")

def main() -> None:
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--repo", help="owner/repo")
    group.add_argument("--repos-file", type=Path)
    parser.add_argument("--metrics", default="openrank,activity,attention", help="comma-separated metrics or 'all'")
    parser.add_argument(
        "--backfill-ho",
        action="store_true",
        help="(deprecated) backfill health_overview_daily; now enabled by default",
    )
    parser.add_argument(
        "--no-backfill-ho",
        action="store_true",
        help="disable automatic backfill of health_overview_daily",
    )
    parser.add_argument("--limit-months", type=int, default=None, help="limit months per repo for backfill")
    parser.add_argument("--state-file", type=Path)
    parser.add_argument("--resume", action="store_true")
    args = parser.parse_args()

    init_db()

    # è§£æ metrics (å¤„ç† 'all')
    metrics = _parse_metrics(args.metrics)
    
    # æ­¤æ—¶ metrics å·²ç»æ˜¯å®Œæ•´çš„åˆ—è¡¨äº†ï¼Œå¯ä»¥ç›´æ¥ç¡®ä¿æ”¯æŒ
    ensure_supported(metrics)

    auto_backfill = not args.no_backfill_ho  # default: on

    if args.repo:
        print(f"ğŸš€ æ­£åœ¨å¤„ç† {args.repo} (å…± {len(metrics)} ä¸ªæŒ‡æ ‡)...")
        counts = fetch_metrics(args.repo, metrics)
        print(f"âœ… å®Œæˆ: {counts}")
        if auto_backfill or args.backfill_ho:
            try:
                backfill_health_overview(args.repo, metrics, limit_months=args.limit_months)
            except Exception as e:
                print(f"   âš ï¸ å›å¡« health_overview_daily å¤±è´¥: {e}")
        try:
            sync_repo_table(args.repo, metrics)
        except Exception as e:
            print(f"   âš ï¸ åŒæ­¥ per-repo è¡¨å¤±è´¥: {e}")
        return

    repos_file: Path = args.repos_file
    resume_marker = _load_resume_marker(args.state_file, args.resume)
    skipping = resume_marker is not None
    
    for repo in _iter_repos(repos_file):
        if skipping:
            if repo == resume_marker: skipping = False
            continue
        print(f"ğŸš€ æ­£åœ¨å¤„ç† {repo}...")
        counts = fetch_metrics(repo, metrics)
        _store_resume_marker(args.state_file, repo)
        print(f"   -> {counts}")
        if auto_backfill or args.backfill_ho:
            try:
                backfill_health_overview(repo, metrics, limit_months=args.limit_months)
            except Exception as e:
                print(f"   âš ï¸ å›å¡« health_overview_daily å¤±è´¥: {e}")
        try:
            sync_repo_table(repo, metrics)
        except Exception as e:
            print(f"   âš ï¸ åŒæ­¥ per-repo è¡¨å¤±è´¥: {e}")

if __name__ == "__main__":
    main()